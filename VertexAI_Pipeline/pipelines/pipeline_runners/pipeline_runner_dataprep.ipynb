{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d27bb63-0c9e-4ed9-8ae5-42a587c05efa",
   "metadata": {},
   "source": [
    "# E2E Vertex AI Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14207caf-d7b0-45b1-a188-2abb47171e02",
   "metadata": {},
   "source": [
    "## Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ebb3fc-2ed5-4eb7-aaaa-e34586c38d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "from kfp.v2 import compiler, dsl\n",
    "from functools import partial\n",
    "from jinja2 import Template\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "from google_cloud_pipeline_components.v1.dataflow import DataflowPythonJobOp\n",
    "from google_cloud_pipeline_components.v1.wait_gcp_resources import WaitGcpResourcesOp\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a8924b-4fb2-4491-b9f0-5c910926c7b2",
   "metadata": {},
   "source": [
    "## Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f9fc65b-7310-42b1-8084-7ad305052df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "lemmatize_flag = \"True\"\n",
    "dim_reduction_algo = \"PCA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b87153dc-0d45-4efc-ab82-23d9366c1b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"quantiphi-buzzwords\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "job_id = f\"e2e-training-pipeline-{timestamp}\"\n",
    "region = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b239703-2bc7-4c57-bbc9-a77d41917b6b",
   "metadata": {},
   "source": [
    "## Loading Custom Component\n",
    "\n",
    "\n",
    "Specifying the component directory to load the Data Ingestion Component using component.yaml.jinja file\n",
    "\n",
    "The component.yaml.jinja file contains:\n",
    "\n",
    "- Inputs and Outputs of the component\n",
    "- Artifact registry path of container image\n",
    "- Entrypoint command of docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f81899f3-91a3-41b9-89c1-2becf28cf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_custom_component(components_dir: str, component_name: str) -> kfp.components:\n",
    "    component_path = os.path.join(\n",
    "        components_dir, component_name, \"component.yaml.jinja\"\n",
    "    )\n",
    "    with open(component_path, \"r\") as f:\n",
    "        component_text = Template(f.read()).render()\n",
    "    return kfp.components.load_component_from_text(component_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9520b2f-f534-4d75-8ae9-557a982877d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "components_dir = \"../../components/\"  # \"../components/\" or \"../../components\"\n",
    "load_custom_component = partial(_load_custom_component, components_dir=components_dir)\n",
    "data_prep_op = load_custom_component(component_name=\"data_prep\")\n",
    "training_op = load_custom_component(component_name=\"training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5201e35-819c-429e-9e8f-06b30fc49d3f",
   "metadata": {},
   "source": [
    "## Defining the E2E Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6110f17-6657-48ba-a324-3f7baa446407",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name=\"e2e-training-pipeline\")\n",
    "def pipeline(\n",
    "    sample_size : int,\n",
    "    lemmatize_flag : str,\n",
    "    job_id : str,\n",
    "    dim_reduction_algo : str\n",
    "):\n",
    "\n",
    "    data_prep_task = (data_prep_op(sample_size=sample_size,lemmatize_flag=lemmatize_flag,job_id=job_id,dim_reduction_algo=dim_reduction_algo))\n",
    "    training_task = (training_op(job_id=job_id)).after(data_prep_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879cb4c-90b5-457c-9a24-75d5d943875a",
   "metadata": {},
   "source": [
    "## Compiling Pipeline\n",
    "\n",
    "The pipeline is compiled using the Compiler and all the relevant information is saved in JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c015938-d055-474a-ad80-65686b28bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"../pipeline_spec/e2e_training_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eb3a3e-fa38-492b-9731-45ad4c4af160",
   "metadata": {},
   "source": [
    "## Defining the PipelineJob\n",
    "\n",
    "The PipelineJob is defined which includes pipeline related details(job id, display-name), pipeline root folder to store the pipeline artifacts/metrics, location details and compiled JSON file to run the pipelinejob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a50db055-2b5f-4526-96f1-4ea1b05dcd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    display_name=\"e2e-training-pipeline\",\n",
    "    template_path=\"../pipeline_spec/e2e_training_pipeline.json\",\n",
    "    job_id=job_id,\n",
    "    parameter_values={\n",
    "        \"sample_size\": sample_size,\n",
    "        \"lemmatize_flag\": lemmatize_flag,\n",
    "        \"job_id\":job_id,\n",
    "        \"dim_reduction_algo\":dim_reduction_algo,\n",
    "    },\n",
    "    location=region,\n",
    "    enable_caching=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd15b1-d8ad-4a18-bb20-9313da1a8793",
   "metadata": {},
   "source": [
    "submitting pipeline job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f9a1-8752-441f-9f05-44e6092c8062",
   "metadata": {},
   "source": [
    "## Submitting the Pipeline job\n",
    "\n",
    "The Pipelinejob is submitted using the submit() and logs of the job can be viewed on the VertexAI platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5b2f556-f193-4e36-afcf-a4af1d92b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/431547520072/locations/us-central1/pipelineJobs/e2e-training-pipeline-20221110072122\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/431547520072/locations/us-central1/pipelineJobs/e2e-training-pipeline-20221110072122')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/e2e-training-pipeline-20221110072122?project=431547520072\n"
     ]
    }
   ],
   "source": [
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa6c822-9900-4b13-8de2-caed79712a96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
