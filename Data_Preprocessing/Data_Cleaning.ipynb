{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMOcpKDqirU2"
   },
   "source": [
    "# **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsQcT1awjYil"
   },
   "source": [
    "Pre-processing the data is the process of cleaning and preparing the text for classification. Online texts contain usually\n",
    "lots of noise and uninformative parts such as HTML tags, scripts and advertisements. In addition, on words level, many\n",
    "words in the text do not have an impact on the general orientation of it.\n",
    "Keeping those words makes the dimensionality of the problem high and hence the classification more difficult since each\n",
    "word in the text is treated as one dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kgfpHG8zcos"
   },
   "source": [
    "## **1. Text Cleaning / Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_gZkclAE_CN"
   },
   "source": [
    " - Text normalization reduces variations in word forms to a common form when the variations mean the same thing\n",
    " - Before text data is used in training NLP models, it's pre-processed to a suitable form. Text normalization is often an essential step in text pre-processing. Text normalization simplifies the modelling process and can improve the model's performance.\n",
    " - Some of the Text Normalization techniques include :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqvOkkFNFkSH"
   },
   "source": [
    "### **A. Removing White Spaces**\n",
    "\n",
    "Most of the time the text data that you have may contain extra spaces in between the words, after or before a sentence. So to start with we will remove these extra spaces from each sentence by using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "v_R9zWRky6ke",
    "outputId": "d8d99059-1554-40e3-e51d-a6c20694aaf0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' My office is located in Mumbai'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing all white spaces using regex\n",
    "import re\n",
    "string = \" My office is    located in Mumbai\"\n",
    "new_str = re.sub(r\"\\s+\",\" \", string)\n",
    "new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HONY9FHdHstw",
    "outputId": "a1c22a87-a137-4d89-cf2c-802a7298783b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'My office is located in Mumbai'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing Leading or trailing whitespaces using strip\n",
    "string = \"   My office is located in Mumbai   \"\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fvk0TJkBIkcy"
   },
   "source": [
    "### **B. Removing punctuations and special characters**\n",
    "\n",
    "\n",
    "The punctuations present in the text do not add value to the data. The punctuation, when attached to any word, will create a problem in differentiating with other words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UBpvVPSRi5ZN",
    "outputId": "33c82c42-84cd-4b6f-c625-1b5e44574f71"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hey All I am going to London'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "txt = \"Hey All, I am going to London!!@@\"\n",
    "clean_txt = \"\".join([i for i in txt if i not in string.punctuation])\n",
    "clean_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhJOGDsYJ6WM"
   },
   "source": [
    "### **C. Case Normalization**\n",
    "\n",
    "Case Normalization is essential so that machine models donâ€™t group capitalized words (HELLo) as different from their lowercase counterparts (hello)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HIu90BkyKA41",
    "outputId": "617abc55-f144-4473-e178-9ac7843121ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hey all, i am going to london for bumble project!!@@'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to lower case\n",
    "txt = \"Hey All, I am going to London for Bumble project!!@@\"\n",
    "txt.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "X_KIS36AKz1T",
    "outputId": "b99f5793-eaae-4535-84bc-84c7ce948fc4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'HEY ALL, I AM GOING TO LONDON FOR BUMBLE PROJECT!!@@'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Converting to upper case\n",
    "txt = \"Hey All, I am going to London for Bumble project!!@@\"\n",
    "txt.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS3qKB-1LCYF"
   },
   "source": [
    "### **D. Removing Stopwords**\n",
    "\n",
    "Stopwords include: I, he, she, and, but, was were, being, have, etc, which do not add meaning to the data. So these words must be removed which helps to reduce the features from our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "983TVUoNPvUd"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pdUCgNFEMlgY",
    "outputId": "bb973dec-8c5e-434a-a5fc-7625777e6293"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stop = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "iQjgX3wmLaMB",
    "outputId": "0d9d1526-aeb2-4fc9-a6d1-3a5aadb201bd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Hey All I going London summer'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hey All I am going to London this summer\"\n",
    "text_new = \" \".join([i for i in text.split() if i not in stop])\n",
    "text_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcMlI-jEOHda"
   },
   "source": [
    "### E. **Lemmatization & Stemming**\n",
    "\n",
    "**Stemming:** A technique that takes the word to its root form. It just removes suffixes from the words. The stemmed word might not be part of the dictionary, i.e it will not necessarily give meaning\n",
    "\n",
    "**Lemmatization:** Takes the word to its root form called Lemma. It helps to bring words to their dictionary form. It is applied to nouns by default. It is more accurate as it uses more informed analysis to create groups of words with similar meanings based on the context, so it is complex and takes more time. This is used where we need to retain the contextual information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "FHA3t7fBOyUC"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYQdSa2qPSmw",
    "outputId": "93064e84-1154-4838-82b0-2df648eb5375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer Outputs\n",
      "program\n",
      "chang\n",
      "troubl\n",
      "mug\n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "print(\"Porter Stemmer Outputs\")\n",
    "print(porter.stem(\"programming\"))\n",
    "print(porter.stem(\"changing\"))\n",
    "print(porter.stem(\"troubling\"))\n",
    "print(porter.stem(\"mugged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJinieZQPjbI",
    "outputId": "b67a053f-1957-427a-d819-134bf4629e69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancaster Stemmer Outputs\n",
      "program\n",
      "chang\n",
      "troubl\n",
      "mug\n"
     ]
    }
   ],
   "source": [
    "lancaster=LancasterStemmer()\n",
    "print(\"Lancaster Stemmer Outputs\")\n",
    "print(lancaster.stem(\"programming\"))\n",
    "print(lancaster.stem(\"changing\"))\n",
    "print(lancaster.stem(\"troubling\"))\n",
    "print(lancaster.stem(\"mugged\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAFIQAEBP4hN",
    "outputId": "db41c66c-a525-4dae-e14b-8a3e8d7ee021"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtDHVRZZQRww",
    "outputId": "a915a34d-8b4b-4bf1-8d47-4844c5952c6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "programming\n",
      "changing\n",
      "troubling\n",
      "mugged\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize(\"programming\"))\n",
    "print(wordnet_lemmatizer.lemmatize(\"changing\"))\n",
    "print(wordnet_lemmatizer.lemmatize(\"troubling\"))\n",
    "print(wordnet_lemmatizer.lemmatize(\"mugged\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blvso4bcPhQO"
   },
   "source": [
    "In the above output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. You need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MQlORzmFRVC-",
    "outputId": "b48b7e19-05a9-4bed-fb6c-ca672c404e8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program\n",
      "change\n",
      "trouble\n",
      "London\n"
     ]
    }
   ],
   "source": [
    "print(wordnet_lemmatizer.lemmatize(\"programming\",pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize(\"changing\",pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize(\"troubling\",pos='v'))\n",
    "print(wordnet_lemmatizer.lemmatize(\"London\",pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0zOj1nFSLeo"
   },
   "source": [
    "#### **Stemming or Lemmatization?**\n",
    "\n",
    "- Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "- Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faGUY5SXTJOJ"
   },
   "source": [
    "### **F. Spelling Correction:**\n",
    "\n",
    "In some applications such as Information Retrieval, it's useful to correct spelling errors. For example, 'infromation' is normalized to 'information'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cui_kmisSivx",
    "outputId": "4c2399d0-916d-4b13-ede6-efef302c3bb6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I am going to London for an important project\")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "text = \"I am ging to London f0r an importnt projct\"\n",
    "textBlb = TextBlob(text)            # Making our first textblob\n",
    "textCorrected = textBlb.correct()   # Correcting the text\n",
    "textCorrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOjGkbPkVQSw"
   },
   "source": [
    "### **Clean Text Library for Text Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIKlgowxVqFh",
    "outputId": "b4b06aef-3571-42ca-8d59-58d7e8370461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting clean-text\n",
      "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
      "Collecting emoji<2.0.0,>=1.0.0\n",
      "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting ftfy<7.0,>=6.0\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 1.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy<7.0,>=6.0->clean-text) (0.2.5)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=e34af865f9308049d12e6a74791ea6c29570fa8ef66386fc8428a32744a093ce\n",
      "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
      "Successfully built emoji\n",
      "Installing collected packages: ftfy, emoji, clean-text\n",
      "Successfully installed clean-text-0.6.0 emoji-1.7.0 ftfy-6.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install clean-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8lr_MG_VfgQ",
    "outputId": "41a4867d-566c-498e-8273-9ff6dfd7012e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "L8yKgh35VVbl",
    "outputId": "d31645ef-5e3e-40b2-b686-9886b4b2040b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'my email id is <email> and the url for my website is <url> my phone number is +<phone> the parking in new york costs <cur> per hour'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\"My email id is adi@gmail.com and the url for my website is https://quantiphi.com. My phone number is +91799331115 @@@. The parking in New York costs $50 per hour\",\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"                      \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB8mZaUqXn-k"
   },
   "source": [
    "### **G. Tokenization**\n",
    "\n",
    " - Tokenization is the first step in any NLP pipeline. It has an important effect on the rest of your pipeline. A tokenizer breaks unstructured data and natural language text into chunks of information that can be considered as discrete elements. The token occurrences in a document can be used directly as a vector representing that document. \n",
    "\n",
    "- Tokenization can separate sentences, words, characters, or subwords. When we split the text into sentences, we call it sentence tokenization. For words, we call it word tokenization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bppNLQ_dYlZ2"
   },
   "source": [
    "#### White Space Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cg-U7PYoYioj",
    "outputId": "cfaae3a7-6d5b-47b0-bbce-8d5611d9a9d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'work', 'as', 'a', 'Machine', 'Learning', 'Engineer']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"I work as a Machine Learning Engineer\"\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYGB5xEyY81K"
   },
   "source": [
    "In the example below, weâ€™ll perform sentence tokenization using the comma as a separator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4z9-KO56XrQl",
    "outputId": "311dd0c7-b7e2-42ce-e8df-2bea1a0db077"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I work as a Machine Learning Engineer', ' My office is in Mumbai']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = \"I work as a Machine Learning Engineer. My office is in Mumbai\"\n",
    "sents.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DHF1baAaOOb"
   },
   "source": [
    "#### NLTK based tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3u144-uaV8U",
    "outputId": "2ce8f5e0-f2ac-45a5-a741-84a664c53034"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, wordpunct_tokenize, TweetTokenizer, MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "yspHEaMIbHWF"
   },
   "outputs": [],
   "source": [
    "text = \"I work as a Machine Learning Engineer.  My office is in Mumbai.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj6PguxHaTCJ"
   },
   "source": [
    "Word and Sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ABdS76s5ZMye",
    "outputId": "44771c00-88be-4b8f-d709-1317eb39fb28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'work', 'as', 'a', 'Machine', 'Learning', 'Engineer', '.', 'My', 'office', 'is', 'in', 'Mumbai', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzcBqAn0b6_c",
    "outputId": "e9695c4a-cc81-43ae-a671-4a624cd86fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I work as a Machine Learning Engineer.', 'My office is in Mumbai.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKBQ-YYGcBEL"
   },
   "source": [
    "#### Punctuation-based tokenizer\n",
    "This tokenizer splits the sentences into words based on whitespaces and punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thhwvhf5cHFl",
    "outputId": "dbe66cba-35cf-4b17-c8cd-9e9a1311283e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'work', 'as', 'a', 'Machine', 'Learning', 'Engineer', '.', 'My', 'office', 'is', 'in', 'Mumbai', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDsqDhvoci43"
   },
   "source": [
    "#### Tweet tokenizer\n",
    "When we want to apply tokenization in text data like tweets, the tokenizers mentioned above canâ€™t produce practical tokens. Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need them for tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dlZNXVyqcpFw",
    "outputId": "03a7f154-4d53-4755-fdbf-deb5c6b2d680"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'ðŸ˜‚', 'ðŸ˜‚', 'ðŸ™Œ', 'ðŸ˜†']\n"
     ]
    }
   ],
   "source": [
    "text = \"HelloðŸ˜‚ðŸ˜‚ðŸ™ŒðŸ˜†\"\n",
    "tok = TweetTokenizer()\n",
    "print(tok.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tkh9vPAHdmFc"
   },
   "source": [
    "#### MWET tokenizer\n",
    "\n",
    "NLTKâ€™s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows the user to enter multiple word expressions before using the tokenizer on the text. More simply, it can merge multi-word expressions into single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4mq04fEpd_y6",
    "outputId": "b9d375c9-bcb0-4139-f54f-642ce92e8bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'go', 'to', 'New', 'York']\n"
     ]
    }
   ],
   "source": [
    "text = \"I want to go to New York\"\n",
    "tok = MWETokenizer()\n",
    "print(tok.tokenize(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA8r2JIkeTOB",
    "outputId": "86086471-08f9-489a-ae43-ee9ee6b2c874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'want', 'to', 'go', 'to', 'New_York']\n"
     ]
    }
   ],
   "source": [
    "text = \"I want to go to New York\"\n",
    "tok = MWETokenizer()\n",
    "tok.add_mwe((\"New\",\"York\"))\n",
    "print(tok.tokenize(word_tokenize(text)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
