{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction**"
      ],
      "metadata": {
        "id": "KpyVxQDsyqeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering is a very key part of Natural Language Processing. as we all know algorithms and machines can’t understand characters or words or sentences hence we need to encode these words into some specific form of numerical in order to interact with algorithms or machines. we can’t feed the text data containing words or sentences or characters to a machine learning model."
      ],
      "metadata": {
        "id": "bnyiPv864I9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Bag of Words(BOW) model**\n",
        "\n",
        "It’s the simplest model, Imagine a sentence as a bag of words here The idea is to take the whole text data and count their frequency of occurrence. and map the words with their frequency. This method doesn’t care about the order of the words, but it does care how many times a word occurs and the default bag of words model treats all words equally.\n",
        "\n",
        "The feature vector will have the same word length. Words that come multiple times get higher weightage making this model biased"
      ],
      "metadata": {
        "id": "1E9blNfD4Y9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBIu-Nly4Hpl",
        "outputId": "890e4e35-073f-40a5-ed9d-c6eeb8aba331"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=1b86da812f36887f617436e76db71a0ae350cb983385ef3f48ad110ee74e740c\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I work in Mumbai\",\n",
        "        \"NLP is a niche skill\",\n",
        "        \"I will travel to London in a month\"]\n",
        "vectorizer = CountVectorizer()\n",
        "count_matrix = vectorizer.fit_transform(text)\n",
        "count_array = count_matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfl3RAin4ySn",
        "outputId": "285bd328-6a6a-4c50-f6d5-82d79f5fb300"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   in  is  london  month  mumbai  niche  nlp  skill  to  travel  will  work\n",
            "0   1   0       0      0       1      0    0      0   0       0     0     1\n",
            "1   0   1       0      0       0      1    1      1   0       0     0     0\n",
            "2   1   0       1      1       0      0    0      0   1       1     1     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After fitting the countVectorizer we can transform any text into the fitted vocabulary."
      ],
      "metadata": {
        "id": "TYpXVP2k5kLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = ['I love to travel to London, but I prefer to stay in mumbai']\n",
        "print(vectorizer.transform(text2).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUSh5N_r5lfe",
        "outputId": "72b289de-4e3b-4456-8312-a3d18852fec2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 0 1 0 0 0 3 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Implementation of the BOW model with n-gram:**\n",
        "\n",
        "assume that we have the word “not bad” and if we split this into “not” and “bad” then it will lose out its meaning. “not bad” is similar to “good” to some extent. we don’t want to split such words which lose their meaning after splitting. here the idea of n-grams comes into the picture."
      ],
      "metadata": {
        "id": "BaYfqVDT68D5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I work in Mumbai\",\n",
        "        \"NLP is a niche skill\",\n",
        "        \"I will travel to London in a month\"]\n",
        "vectorizer = CountVectorizer(ngram_range = (1,2)) #ngram_range =(1, 1) means only unigrams, ngram_range = (1, 2) means unigrams with bigrams ngram_range=(2, 2) means only bigrams.\n",
        "count_matrix = vectorizer.fit_transform(text)\n",
        "count_array = count_matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaHydww967ZI",
        "outputId": "ebb4a213-761b-43b5-bffc-beb45434ac10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   in  in month  in mumbai  is  is niche  london  london in  month  mumbai  \\\n",
            "0   1         0          1   0         0       0          0      0       1   \n",
            "1   0         0          0   1         1       0          0      0       0   \n",
            "2   1         1          0   0         0       1          1      1       0   \n",
            "\n",
            "   niche  ...  nlp is  skill  to  to london  travel  travel to  will  \\\n",
            "0      0  ...       0      0   0          0       0          0     0   \n",
            "1      1  ...       1      1   0          0       0          0     0   \n",
            "2      0  ...       0      0   1          1       1          1     1   \n",
            "\n",
            "   will travel  work  work in  \n",
            "0            0     1        1  \n",
            "1            0     0        0  \n",
            "2            1     0        0  \n",
            "\n",
            "[3 rows x 22 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text2 = ['I love to travel to London, but I prefer to stay in mumbai']\n",
        "print(vectorizer.transform(text2).toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2aTNk3jFmAn",
        "outputId": "507c656f-c85d-48d0-88a8-839ad27b2a83"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 1 0 0 1 0 0 1 0 0 0 0 0 3 1 1 1 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The BOW model doesn’t give good results since it has a drawback. Assume that there is a particular word that is appearing in all the documents and it comes multiple times, eventually, it will have a higher frequency of occurrence and it will have a greater value that will cause a specific word to have more weightage in a sentence, that’s not good for our analysis."
      ],
      "metadata": {
        "id": "DZc2XTEi8PVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.TF-IDF (Term frequency-inverse Document Frequency)**\n",
        "\n",
        "The idea of TF-IDF is to reflect the importance of a word to its document or sentence by normalizing the words which occur frequently in the collection of documents.\n",
        "\n",
        "**Term frequency (TF):** number of times a term has appeared in a document.\n",
        "\n",
        "The term frequency is a measure of how frequently or how common a word is for a given sentence.\n",
        "\n",
        "**Inverse Document Frequency (IDF):**\n",
        "\n",
        "The inverse document frequency (IDF) is a measure of how rare a word is in a document. Words like “the”,” a” show up in all the documents but rare words will not occur in all the documents of the corpus.\n",
        "\n",
        "If a word appears in almost every document means it’s not significant for the classification.\n",
        "\n",
        "IDF of a word is = log(N/n)\n",
        "\n",
        "N: total number of documents.\n",
        "n: number of documents containing a term (word)\n",
        "\n",
        "TF-IDF Evaluates how relevant is a word to its sentence in a collection of sentences or documents."
      ],
      "metadata": {
        "id": "MsteJF078KjH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manual creation of Tf-idf model from scratch**"
      ],
      "metadata": {
        "id": "1m5P0y9c_NiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('punkt')\n",
        "text = [\"I work in Mumbai\",\n",
        "        \"NLP is a niche skill\",\n",
        "        \"I will travel to London in a month\"]\n",
        "#Preprocessing the text data\n",
        "sentences = []\n",
        "word_set = []\n",
        " \n",
        "for sent in text:\n",
        "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
        "    sentences.append(x)\n",
        "    for word in x:\n",
        "        if word not in word_set:\n",
        "            word_set.append(word)\n",
        " \n",
        "#Set of vocab \n",
        "word_set = set(word_set)\n",
        "#Total documents in our corpus\n",
        "total_documents = len(sentences)\n",
        " \n",
        "#Creating an index for each word in our vocab.\n",
        "index_dict = {} #Dictionary to store index for each word\n",
        "i = 0\n",
        "for word in word_set:\n",
        "    index_dict[word] = i\n",
        "    i += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keLU_01u_Sww",
        "outputId": "3be4d4b3-3649-4c56-e609-be5fb426f4fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a count dictionary\n",
        "def count_dict(sentences):\n",
        "    word_count = {}\n",
        "    for word in word_set:\n",
        "        word_count[word] = 0\n",
        "        for sent in sentences:\n",
        "            if word in sent:\n",
        "                word_count[word] += 1\n",
        "    return word_count\n",
        "word_count = count_dict(sentences)"
      ],
      "metadata": {
        "id": "lvu58mmz_MIc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Term Frequency\n",
        "def termfreq(doc, word):\n",
        "    N = len(doc)\n",
        "    occurance = len([token for token in doc if token == word])\n",
        "    return occurance/N"
      ],
      "metadata": {
        "id": "BR2cBAZ4AGn6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_doc_freq(word):\n",
        "    try:\n",
        "        word_occurance = word_count[word] + 1\n",
        "    except:\n",
        "        word_occurance = 1\n",
        "    return np.log(total_documents/word_occurance)"
      ],
      "metadata": {
        "id": "wiw5WtzGAw-R"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf(sentence):\n",
        "    tf_idf_vec = np.zeros((len(word_set),))\n",
        "    for word in sentence:\n",
        "        tf = termfreq(sentence,word)\n",
        "        idf = inverse_doc_freq(word)\n",
        "         \n",
        "        value = tf*idf\n",
        "        tf_idf_vec[index_dict[word]] = value \n",
        "    return tf_idf_vec"
      ],
      "metadata": {
        "id": "dmFwFJ8X__gg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF-IDF Encoded text corpus\n",
        "import numpy as np\n",
        "import pprint\n",
        "vectors = []\n",
        "for sent in sentences:\n",
        "    vec = tf_idf(sent)\n",
        "    vectors.append(vec)\n",
        " \n",
        "pprint.pprint(vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDlqhaq_VWTK",
        "outputId": "ff1ad767-edc3-4e49-ac3e-3f8d373a959a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
            "        0.        ,  0.        ,  0.        , -0.07192052, -0.07192052,\n",
            "        0.        ,  0.        ,  0.10136628]),\n",
            " array([ 0.08109302,  0.        ,  0.        ,  0.08109302,  0.        ,\n",
            "        0.        ,  0.08109302,  0.        , -0.05753641, -0.05753641,\n",
            "        0.        ,  0.        ,  0.        ]),\n",
            " array([ 0.        ,  0.04505168,  0.        ,  0.        ,  0.04505168,\n",
            "        0.04505168,  0.        ,  0.04505168, -0.03196467, -0.03196467,\n",
            "        0.04505168,  0.04505168,  0.        ])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tf-idf model using sklearn**"
      ],
      "metadata": {
        "id": "tDLKACEfP-BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "gMQVT09C53S3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"I work in Mumbai\",\n",
        "        \"NLP is a niche skill to work on\",\n",
        "        \"I will travel to London for work in a month\",\n",
        "        \"I came to work here on nlp\"]\n",
        "vectorizer = TfidfVectorizer()\n",
        "matrix = vectorizer.fit_transform(text)\n",
        "count_array = matrix.toarray()\n",
        "df = pd.DataFrame(data=count_array,columns = vectorizer.get_feature_names_out())\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5VRK-Pv9JIQ",
        "outputId": "aadb6b18-5918-457e-a060-84c1157b1153"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       came       for      here        in        is    london     month  \\\n",
            "0  0.000000  0.000000  0.000000  0.572892  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.000000  0.000000  0.000000  0.450701  0.000000  0.000000   \n",
            "2  0.000000  0.398368  0.000000  0.314078  0.000000  0.398368  0.398368   \n",
            "3  0.504889  0.000000  0.504889  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "     mumbai     niche       nlp        on     skill        to    travel  \\\n",
            "0  0.726641  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "1  0.000000  0.450701  0.355338  0.355338  0.450701  0.287677  0.000000   \n",
            "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.254273  0.398368   \n",
            "3  0.000000  0.000000  0.398060  0.398060  0.000000  0.322264  0.000000   \n",
            "\n",
            "       will      work  \n",
            "0  0.000000  0.379192  \n",
            "1  0.000000  0.235195  \n",
            "2  0.398368  0.207885  \n",
            "3  0.000000  0.263472  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2Vec**\n",
        "\n",
        "Word2Vec consists of models for generating word embedding. These models are shallow two-layer neural networks having one input layer, one hidden layer, and one output layer. Word2Vec utilizes two architectures :\n",
        "\n",
        "Learning Links : \n",
        "- https://www.youtube.com/watch?v=UqRCEmrv1gQ\n",
        "- https://www.youtube.com/watch?v=Otde6VGvhWM\n"
      ],
      "metadata": {
        "id": "CqUkvwh_K299"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbGXJdftMA2Z",
        "outputId": "0f9d16cd-ee5c-4128-a2f0-bc6192f6112f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **1.CBOW (Continuous Bag of Words):**\n",
        "CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. "
      ],
      "metadata": {
        "id": "aYa_DPPeLNma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82dwc5iuNyPr",
        "outputId": "8f26c4f6-166d-4476-e0a4-4313bc159c72"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all necessary modules\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Reads txt file\n",
        "sample = open(\"word2vec_training_data_aliceinwonderland.txt\")\n",
        "\n",
        "s = sample.read()\n",
        "\n",
        "# Replaces escape character with space\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "\n",
        "data = []\n",
        "\n",
        "# iterate through each sentence in the file\n",
        "for i in sent_tokenize(f):\n",
        "\ttemp = []\n",
        "\t# tokenize the sentence into words\n",
        "\tfor j in word_tokenize(i):\n",
        "\t\ttemp.append(j.lower())\n",
        "\n",
        "\tdata.append(temp)\n",
        "\n",
        "# Create CBOW model\n",
        "model1 = gensim.models.Word2Vec(data, min_count = 1, window = 5)\n",
        "\n",
        "# Print results\n",
        "print(\"Cosine similarity between 'alice' \" +\n",
        "\t\t\t\"and 'wonderland' - CBOW : \",\n",
        "\tmodel1.wv.similarity('alice', 'wonderland'))\n",
        "\t\n",
        "print(\"Cosine similarity between 'alice' \" +\n",
        "\t\t\t\t\"and 'machines' - CBOW : \",\n",
        "\tmodel1.wv.similarity('alice', 'machines'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psgeXmqRLCCo",
        "outputId": "783e3155-8767-4813-b09e-675e1b496b9c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.98836577\n",
            "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9476097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.Skip Gram:** \n",
        "Skip gram predicts the surrounding context words within specific window given current word. The input layer contains the current word and the output layer contains the context words. The hidden layer contains the number of dimensions in which we want to represent current word present at the input layer. "
      ],
      "metadata": {
        "id": "BT6DSBU3LxxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Skip Gram model\n",
        "model2 = gensim.models.Word2Vec(data, min_count = 1,window = 5, sg = 1)\n",
        " \n",
        "# Print results\n",
        "print(\"Cosine similarity between 'alice' \" +\n",
        "          \"and 'wonderland' - Skip Gram : \",\n",
        "    model2.wv.similarity('alice', 'wonderland'))\n",
        "     \n",
        "print(\"Cosine similarity between 'alice' \" +\n",
        "            \"and 'machines' - Skip Gram : \",\n",
        "      model2.wv.similarity('alice', 'machines'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LD0Bh7mIL5rU",
        "outputId": "43d69a96-cfa3-489f-c772-f672a27632e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.6384189\n",
            "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8152881\n"
          ]
        }
      ]
    }
  ]
}